---
title: "Sequence to sequence unsynchronised 2nd attempt, really unsynchronised"
author: "Dimitri Fichou"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Recurrent Neural Network}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Package

The package is loaded using:

```{r package}
library(rnn)
library(DLC)
```

```{r function_modify}
trainr <- function(Y, X, learningrate, learningrate_decay = 1, momentum = 0, hidden_dim, numepochs = 1, start_from_end=FALSE,unsync=F) {
  
  # extract the time dimensions, needed because if unsync, we break it later
  time_x_dim = dim(X)[2]
  time_y_dim = dim(Y)[2]
  
  # check the consistency
  if(time_x_dim != time_y_dim & unsync == F){
    stop("The time dimension of X is different from the time dimension of Y. if unsync sequence learning is the target, set unsync to T")
  }
  if(dim(X)[1] != dim(Y)[1]){
    stop("The sample dimension of X is different from the sample dimension of Y.")
  }
    if(unsync == T & start_from_end == T){
    stop("could not start from the middle, i.e. unsync and start_from_end are not compatible")
  }
  
  # coerce to array if matrix
  if(length(dim(X)) == 2){
    X <- array(X,dim=c(dim(X),1))
  }
  if(length(dim(Y)) == 2){
    Y <- array(Y,dim=c(dim(Y),1))
  }
  
  if(unsync == T){
    binary_dim = dim(X)[2]+dim(Y)[2] - 1
    # Storing layers states
    store_output <- array(0,dim = dim(Y))
    store_hidden <- array(0,dim = c(dim(Y)[1],binary_dim,hidden_dim))
  }else{
    binary_dim = dim(X)[2]
    # Storing layers states
    store_output <- array(0,dim = dim(Y))
    store_hidden <- array(0,dim = c(dim(Y)[1:2],hidden_dim))
  }
  
  # extract the network dimensions
  input_dim = dim(X)[3]
  output_dim = dim(Y)[3]
  
  
  # initialize neural network weights
  synapse_0 = matrix(stats::runif(n = input_dim*hidden_dim, min=-0.1,max=0.1), nrow=input_dim)
  synapse_1 = matrix(stats::runif(n = hidden_dim*output_dim, min=-0.1,max=0.1), nrow=hidden_dim)
  synapse_h = matrix(stats::runif(n = hidden_dim*hidden_dim, min=-0.1,max=0.1), nrow=hidden_dim)
  
  # initialize the update
  synapse_0_update = matrix(0, nrow = input_dim, ncol = hidden_dim)
  synapse_1_update = matrix(0, nrow = hidden_dim, ncol = output_dim)
  synapse_h_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)
  
  
  
  
  # Storing errors, dim 1: samples, dim 2 is epochs, we could store also the time and variable dimension
  error <- array(0,dim = c(dim(Y)[1],numepochs))
  
  # training logic
  for(epoch in seq(numepochs)){
    message(paste0("Training epoch: ",epoch," - Learning rate: ",learningrate))
    for (j in 1:dim(Y)[1]) {
      
      # generate a simple addition problem (a + b = c)
      a = array(X[j,,],dim=c(dim(X)[2],input_dim))
      
      # true answer
      c = array(Y[j,,],dim=c(dim(Y)[2],output_dim))
      
      overallError = 0
      
      layer_2_deltas = matrix(0,nrow=1, ncol = output_dim)
      layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)
      # layer_1_values = rbind(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim))
      
      # time index vector, needed because we predict in one direction but update the weight in an other
      if(start_from_end == TRUE) {
        pos_vec      <- binary_dim:1
        pos_vec_back <- 1:binary_dim
      } else {
        pos_vec      <- 1:binary_dim
        pos_vec_back <- binary_dim:1
      }
      
      # moving along the time
      for (position in pos_vec) {
        
        if(unsync == F){
          # generate input and output
          x = a[position,]
          y = c[position,]
          
          # hidden layer (input ~+ prev_hidden)
          layer_1 = sigmoid::logistic((x%*%synapse_0) + (layer_1_values[dim(layer_1_values)[1],] %*% synapse_h))
          
          # output layer (new binary representation)
          layer_2 = sigmoid::logistic(layer_1 %*% synapse_1)
          
          # did we miss?... if so, by how much?
          layer_2_error = y - layer_2
          layer_2_deltas = rbind(layer_2_deltas, layer_2_error * sigmoid::sigmoid_output_to_derivative(layer_2))
          overallError = overallError + sum(abs(layer_2_error))
          
          # storing
          store_output[j,position,] = layer_2
          store_hidden[j,position,] = layer_1
          
          # store hidden layer so we can print it out. Needed for error calculation and weight iteration
          layer_1_values = rbind(layer_1_values, layer_1)
        }else{
          if(position <=  time_x_dim){ # only for the input sequence, we just calcul the hidden state
            x = a[position,]
            
            # hidden layer (input ~+ prev_hidden)
            layer_1 = sigmoid::logistic((x%*%synapse_0) + (layer_1_values[dim(layer_1_values)[1],] %*% synapse_h))
          }
          if(position >=  time_x_dim){ # only for the output sequence, we calcul the hidden state and the output state, note that there is overlapping for one time step
            y = c[position - time_x_dim +1,]
            
            # hidden layer (only ~+ prev_hidden)
            layer_1 = sigmoid::logistic((layer_1_values[dim(layer_1_values)[1],] %*% synapse_h))
            
            # output layer (new binary representation)
            layer_2 = sigmoid::logistic(layer_1 %*% synapse_1)
            
            # did we miss?... if so, by how much?
            layer_2_error = y - layer_2
            layer_2_deltas = rbind(layer_2_deltas, layer_2_error * sigmoid::sigmoid_output_to_derivative(layer_2))
            overallError = overallError + sum(abs(layer_2_error))
            
            # storingm the output is only in this part of the sequence
            store_output[j,position - time_x_dim +1,] = layer_2
          }
          # storingm the hidden is all along the sequence
          store_hidden[j,position,] = layer_1
          
          # store hidden layer so we can print it out. Needed for error calculation and weight iteration
          layer_1_values = rbind(layer_1_values, layer_1)
        }
        
        
        
      }
      
      # store errors
      error[j,epoch] <- overallError
      
      future_layer_1_delta = matrix(0, nrow = 1, ncol = hidden_dim)
      
      # Weight iteration,
      for (position in 0:(binary_dim-1)) {
        if(unsync == F){
          x            = a[pos_vec_back[position+1],]
          layer_1      = layer_1_values[dim(layer_1_values)[1]-position,]
          prev_layer_1 = layer_1_values[dim(layer_1_values)[1]-(position+1),]
          
          # error at output layer
          layer_2_delta = layer_2_deltas[dim(layer_2_deltas)[1]-position,]
          # error at hidden layer
          layer_1_delta = (future_layer_1_delta %*% t(synapse_h) + layer_2_delta %*% t(synapse_1)) *
            sigmoid::sigmoid_output_to_derivative(layer_1)
          
          # let's update all our weights so we can try again
          synapse_1_update = synapse_1_update + matrix(layer_1) %*% layer_2_delta
          synapse_h_update = synapse_h_update + matrix(prev_layer_1) %*% layer_1_delta
          synapse_0_update = synapse_0_update + c(x) %*% layer_1_delta # I had to change X as a vector as it is not a matrix anymore, other option, define it as a matrix of dim()=c(1,input_dim)
          
          future_layer_1_delta = layer_1_delta
        }else{
          if(position <=  time_y_dim-1){ # first for the output sequence, need to correct the update the hidden synapse and the output synapse
            layer_1      = layer_1_values[dim(layer_1_values)[1]-position,]
            prev_layer_1 = layer_1_values[dim(layer_1_values)[1]-(position+1),]
            
            # error at output layer
            layer_2_delta = layer_2_deltas[dim(layer_2_deltas)[1]-position,]
            # error at hidden layer
            layer_1_delta = (future_layer_1_delta %*% t(synapse_h) + layer_2_delta %*% t(synapse_1)) *
              sigmoid::sigmoid_output_to_derivative(layer_1)
            
            # let's update all our weights so we can try again
            synapse_1_update = synapse_1_update + matrix(layer_1) %*% layer_2_delta
            synapse_h_update = synapse_h_update + matrix(prev_layer_1) %*% layer_1_delta
            
            future_layer_1_delta = layer_1_delta
          }
          if(position >=  time_y_dim-1){ # then for the input sequence, need to correct the update the hidden synapse and the input synapse
            x            = a[pos_vec_back[position+1],]
            layer_1      = layer_1_values[dim(layer_1_values)[1]-position,]
            prev_layer_1 = layer_1_values[dim(layer_1_values)[1]-(position+1),]
            
            # error at hidden layer, just the hidden to hidden layer error
            layer_1_delta = (future_layer_1_delta %*% t(synapse_h)) * 
              sigmoid::sigmoid_output_to_derivative(layer_1)
            
            # let's update all our weights so we can try again
            synapse_h_update = synapse_h_update + matrix(prev_layer_1) %*% layer_1_delta
            synapse_0_update = synapse_0_update + c(x) %*% layer_1_delta # I had to change X as a vector as it is not a matrix anymore, other option, define it as a matrix of dim()=c(1,input_dim)
            
            future_layer_1_delta = layer_1_delta
          }
        }
      }
      
      # Calculate the real update including learning rate and momentum
      synapse_0_update = synapse_0_update * learningrate 
      synapse_1_update = synapse_1_update * learningrate 
      synapse_h_update = synapse_h_update * learningrate 
      
      # Applying the update
      synapse_0 = synapse_0 + synapse_0_update
      synapse_1 = synapse_1 + synapse_1_update
      synapse_h = synapse_h + synapse_h_update
      
      # Update the learning rate
      learningrate <- learningrate * learningrate_decay
      
      # Initializing the update with the momentum
      synapse_0_update = synapse_0_update * momentum
      synapse_1_update = synapse_1_update * momentum
      synapse_h_update = synapse_h_update * momentum
    }
    # update best guess if error is minimal
    if(colMeans(error)[epoch] <= min(colMeans(error)[1:epoch])){
      store_output_best <- store_output
      store_hidden_best <- store_hidden
    }
    message(paste0("Epoch error: ",colMeans(error)[epoch]))
  }
  
  
  # output object with synapses
  return(list(synapse_0         = synapse_0,
              synapse_1         = synapse_1,
              synapse_h         = synapse_h,
              error             = error,
              store_output      = store_output,
              store_hidden      = store_hidden,
              store_hidden_best = store_hidden_best,
              store_output_best = store_output_best,
              start_from_end    = start_from_end,
              unsync            = unsync,
              time_x_dim        = time_x_dim,
              time_y_dim        = time_y_dim) )
  
}

predictr <- function(model, X, hidden = FALSE, ...) {
  
  # coerce to array if matrix
  if(length(dim(X)) == 2){
    X <- array(X,dim=c(dim(X),1))
  }
  
  # load neural network weights
  synapse_0      = model$synapse_0
  synapse_1      = model$synapse_1
  synapse_h      = model$synapse_h
  start_from_end = model$start_from_end
  unsync         = model$unsync
  time_y_dim     = model$time_y_dim
  time_x_dim     = model$time_x_dim
  
  # extract the network dimensions, we don't store the binary_dim with the same name as it will be changed later, but we still need it to know what we are doing
  input_dim          = dim(synapse_0)[1]
  output_dim         = dim(synapse_1)[2]
  hidden_dim         = dim(synapse_0)[2]
  
  # Storing layers states
  if(unsync == T){
    store_output <- array(0,dim = c(dim(X)[1],time_y_dim,output_dim))
    store_hidden <- array(0,dim = c(dim(X)[1],dim(X)[2]+time_y_dim-1,hidden_dim))
    # extract the binary_dim, needed to be splitted as it is now different
    binary_dim = dim(X)[2]+time_y_dim-1
  }else{
    store_output <- array(0,dim = c(dim(X)[1:2],output_dim))
    store_hidden <- array(0,dim = c(dim(X)[1:2],hidden_dim))
    # extract the binary_dim, needed to be splitted as it is now different
    binary_dim = dim(X)[2]
  }
  
  
  
  for (j in 1:dim(X)[1]) {
    
    # generate a simple addition problem (a + b = c)
    a = array(X[j,,],dim=c(dim(X)[2],input_dim))
    
    
    layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)
    
    # time index vector, needed because we predict in one direction but update the weight in an other
    if(start_from_end == T){
      pos_vec <- binary_dim:1
      pos_vec_back <- 1:binary_dim
    }else{
      pos_vec <- 1:binary_dim
      pos_vec_back <- binary_dim:1
    }
    
    # moving along the time
    for (position in pos_vec) {
      if(unsync == F){
        # generate input and output
        x = a[position,]
        
        # hidden layer (input ~+ prev_hidden)
        layer_1 = sigmoid::sigmoid((x%*%synapse_0) + (layer_1_values[dim(layer_1_values)[1],] %*% synapse_h), ...)
        
        # output layer (new binary representation)
        layer_2 = sigmoid::sigmoid(layer_1 %*% synapse_1, ...)
        
        # storing
        store_output[j,position,] = layer_2
        store_hidden[j,position,] = layer_1
        
        # store hidden layer so we can print it out. Needed for error calculation and weight iteration
        layer_1_values = rbind(layer_1_values, layer_1)
      }else{
        if(position <=  time_x_dim){ # only for the input sequence, we just calcul the hidden state
            x = a[position,]
            
            # hidden layer (input ~+ prev_hidden)
            layer_1 = sigmoid::logistic((x%*%synapse_0) + (layer_1_values[dim(layer_1_values)[1],] %*% synapse_h))
          }
          if(position >=  time_x_dim){ # only for the output sequence, we calcul the hidden state and the output state, note that there is overlapping for one time step
            
            # hidden layer (only ~+ prev_hidden)
            layer_1 = sigmoid::logistic((layer_1_values[dim(layer_1_values)[1],] %*% synapse_h))
            
            # output layer (new binary representation)
            layer_2 = sigmoid::logistic(layer_1 %*% synapse_1)
            
            # storing, the output is only in this part of the sequence
            store_output[j,position - time_x_dim +1,] = layer_2
          }
          # storingm the hidden is all along the sequence
          store_hidden[j,position,] = layer_1
          
          # store hidden layer so we can print it out. Needed for error calculation and weight iteration
          layer_1_values = rbind(layer_1_values, layer_1)
      }
    }
  }

  
  # return output vector
  if(hidden == FALSE){
    # convert to matrix if 2 dimensional
    if(dim(store_output)[3]==1) {
      store_output <- matrix(store_output,
                       nrow = dim(store_output)[1],
                       ncol = dim(store_output)[2])  }
    # return output
    return(store_output)
  }else{
    return(store_hidden)
  }
}

```

Generate data.

```{r data,fig.height=9,fig.width=7}
load("data/Propolis silica_dim_ok.Rdata")
# load("/home/clau/Dropbox/rnn-master/vignettes/data/Propolis silica_dim_ok.Rdata")
X <- data$chrom[c(18,23),,]
Y <- X[,126:1,]
# # X <- abind(X,X[,126:1,],along=2)
# X <- array(cbind(X,X[,126:1,]),dim=c(2,126*2,4))
# par(xaxt="n",yaxt="n",mar=c(0,0,0,0))
# layout(matrix(c(4,4,4,4,1,1,2,3),ncol=1))
# X %>% raster
# abline(v=126,col="red")
# text(x=0,y=1.8,labels = "Blue sample",col="red",pos=4,cex=1.5)
# text(x=0,y=0.8,labels = "Orange sample",col="red",pos=4,cex=1.5)
# chrom.pict(aperm(X,c(2,1,3)),1)
# abline(v=0.5,col="red")
# chrom.pict(aperm(X,c(2,1,3)),2)
# abline(v=0.5,col="red")
# abind(data$chrom,data$chrom[,126:1,],along=2) %>% raster
```


```{r training, message=FALSE}
set.seed(1)
model <- trainr(Y = Y,X = X,learningrate = 0.0001,hidden_dim = 32,numepochs=1000,unsync=T,momentum=0.1)
```

```{r prediction}
# pred <- predictr(model = model,X = X)
# abind(X,pred,along=2) %>% raster
pred <- predictr(model = model,X = X)
par(mfrow=c(2,1))
abind(X,X[,126:1,],along=2) %>% raster
pred %>% raster

```