---
title: "Sequence to sequence unsyncronised"
author: "Dimitri Fichou"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Recurrent Neural Network}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Package

The package is loaded using:

```{r package}
library(rnn)
library(DLC)
```

```{r function_modify}
trainr <- function(Y, X, learningrate, learningrate_decay = 1, momentum = 0, hidden_dim, numepochs = 1, start_from_end=FALSE,unsync=F) {
  
  # extract the time dimensions, needed because if unsync, we break it later
  time_x_dim = dim(X)[2]
  time_y_dim = dim(Y)[2]
  
  # check the consistency
  if(time_x_dim != time_y_dim & unsync == F){
    stop("The time dimension of X is different from the time dimension of Y. if unsync sequence learning is the target, set unsync to T")
  }
  if(dim(X)[1] != dim(Y)[1]){
    stop("The sample dimension of X is different from the sample dimension of Y.")
  }
    if(unsync == T & start_from_end == T){
    stop("could not start from the middle, i.e. unsync and start_from_end are not compatible")
  }
  
  # coerce to array if matrix
  if(length(dim(X)) == 2){
    X <- array(X,dim=c(dim(X),1))
  }
  if(length(dim(Y)) == 2){
    Y <- array(Y,dim=c(dim(Y),1))
  }
  
  if(unsync == T){
    X <- abind(X,Y,along=2) ## Need to avoid abind here, first create the object with 0 and the good dimension, then fill it
    Y <- X[,c(2:dim(X)[2],dim(X)[2]),] ## What we do is to shift the output data by one temporal unit
  }
  
  # extract the network dimensions
  input_dim = dim(X)[3]
  output_dim = dim(Y)[3]
  binary_dim = dim(X)[2]
  
  # initialize neural network weights
  synapse_0 = matrix(stats::runif(n = input_dim*hidden_dim, min=-0.1,max=0.1), nrow=input_dim)
  synapse_1 = matrix(stats::runif(n = hidden_dim*output_dim, min=-0.1,max=0.1), nrow=hidden_dim)
  synapse_h = matrix(stats::runif(n = hidden_dim*hidden_dim, min=-0.1,max=0.1), nrow=hidden_dim)
  
  # initialize the update
  synapse_0_update = matrix(0, nrow = input_dim, ncol = hidden_dim)
  synapse_1_update = matrix(0, nrow = hidden_dim, ncol = output_dim)
  synapse_h_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)
  
  # initialize the old update for the momentum
  synapse_0_old_update = matrix(0, nrow = input_dim, ncol = hidden_dim)
  synapse_1_old_update = matrix(0, nrow = hidden_dim, ncol = output_dim)
  synapse_h_old_update = matrix(0, nrow = hidden_dim, ncol = hidden_dim)

  
  # Storing layers states
  store_output <- array(0,dim = dim(Y))
  store_hidden <- array(0,dim = c(dim(Y)[1:2],hidden_dim))
  
  # Storing errors, dim 1: samples, dim 2 is epochs, we could store also the time and variable dimension
  error <- array(0,dim = c(dim(Y)[1],numepochs))
  
  # training logic
  for(epoch in seq(numepochs)){
    message(paste0("Training epoch: ",epoch," - Learning rate: ",learningrate))
    for (j in 1:dim(Y)[1]) {
      
      # generate a simple addition problem (a + b = c)
      a = array(X[j,,],dim=c(dim(X)[2],input_dim))
      
      # true answer
      c = array(Y[j,,],dim=c(dim(Y)[2],output_dim))
      
      overallError = 0
      
      layer_2_deltas = matrix(0,nrow=1, ncol = output_dim)
      layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)
      # layer_1_values = rbind(layer_1_values, matrix(0, nrow=1, ncol=hidden_dim))
      
      # time index vector, needed because we predict in one direction but update the weight in an other
      if(start_from_end == TRUE) {
        pos_vec      <- binary_dim:1
        pos_vec_back <- 1:binary_dim
      } else {
        pos_vec      <- 1:binary_dim
        pos_vec_back <- binary_dim:1
      }
      
      # moving along the time
      for (position in pos_vec) {
        
        # generate input and output
        x = a[position,]
        y = c[position,]
        
        # hidden layer (input ~+ prev_hidden)
        layer_1 = sigmoid::logistic((x%*%synapse_0) + (layer_1_values[dim(layer_1_values)[1],] %*% synapse_h))
        
        # output layer (new binary representation)
        layer_2 = sigmoid::logistic(layer_1 %*% synapse_1)
        
        # did we miss?... if so, by how much?
        layer_2_error = y - layer_2
        layer_2_deltas = rbind(layer_2_deltas, layer_2_error * sigmoid::sigmoid_output_to_derivative(layer_2))
        overallError = overallError + sum(abs(layer_2_error))
        
        # storing
        store_output[j,position,] = layer_2
        store_hidden[j,position,] = layer_1
        
        # store hidden layer so we can print it out. Needed for error calculation and weight iteration
        layer_1_values = rbind(layer_1_values, layer_1)
        
        # need to add something about the unsync here alos and not just in hte predictr function
        # so after the first sequence, the network get as input the output of the last time step
        if(unsync == T & position >= time_x_dim & position != time_x_dim+time_y_dim){
          a[position+1,] <- layer_2
        }
      }
      
      # store errors
      error[j,epoch] <- overallError
      
      future_layer_1_delta = matrix(0, nrow = 1, ncol = hidden_dim)
      
      # Weight iteration,
      for (position in 0:(binary_dim-1)) {
        
        x            = a[pos_vec_back[position+1],]
        layer_1      = layer_1_values[dim(layer_1_values)[1]-position,]
        prev_layer_1 = layer_1_values[dim(layer_1_values)[1]-(position+1),]
        
        # error at output layer
        layer_2_delta = layer_2_deltas[dim(layer_2_deltas)[1]-position,]
        # error at hidden layer
        layer_1_delta = (future_layer_1_delta %*% t(synapse_h) + layer_2_delta %*% t(synapse_1)) *
          sigmoid::sigmoid_output_to_derivative(layer_1)
        
        # let's update all our weights so we can try again
        synapse_1_update = synapse_1_update + matrix(layer_1) %*% layer_2_delta
        synapse_h_update = synapse_h_update + matrix(prev_layer_1) %*% layer_1_delta
        synapse_0_update = synapse_0_update + c(x) %*% layer_1_delta # I had to change X as a vector as it is not a matrix anymore, other option, define it as a matrix of dim()=c(1,input_dim)
        
        future_layer_1_delta = layer_1_delta
      }
      
      # Calculate the real update including learning rate and momentum
      synapse_0_update = synapse_0_update * learningrate + synapse_0_old_update * momentum
      synapse_1_update = synapse_1_update * learningrate + synapse_1_old_update * momentum
      synapse_h_update = synapse_h_update * learningrate + synapse_h_old_update * momentum
      
      # Applying the update
      synapse_0 = synapse_0 + synapse_0_update
      synapse_1 = synapse_1 + synapse_1_update
      synapse_h = synapse_h + synapse_h_update
      
      # Update the learning rate
      learningrate <- learningrate * learningrate_decay
      
      # Storing the old update for next momentum
      synapse_0_old_update = synapse_0_update
      synapse_1_old_update = synapse_1_update
      synapse_h_old_update = synapse_h_update
      
      # Initializing the update
      synapse_0_update = synapse_0_update * 0
      synapse_1_update = synapse_1_update * 0
      synapse_h_update = synapse_h_update * 0
    }
    # update best guess if error is minimal
    if(colMeans(error)[epoch] <= min(colMeans(error)[1:epoch])){
      store_output_best <- store_output
      store_hidden_best <- store_hidden
    }
    message(paste0("Epoch error: ",colMeans(error)[epoch]))
  }
  
  
  # output object with synapses
  return(list(synapse_0         = synapse_0,
              synapse_1         = synapse_1,
              synapse_h         = synapse_h,
              error             = error,
              store_output      = store_output,
              store_hidden      = store_hidden,
              store_hidden_best = store_hidden_best,
              store_output_best = store_output_best,
              start_from_end    = start_from_end,
              unsync            = unsync,
              time_x_dim        = time_x_dim,
              time_y_dim        = time_y_dim) )
  
}

predictr <- function(model, X, hidden = FALSE, ...) {
  
  # coerce to array if matrix
  if(length(dim(X)) == 2){
    X <- array(X,dim=c(dim(X),1))
  }
  
  # load neural network weights
  synapse_0      = model$synapse_0
  synapse_1      = model$synapse_1
  synapse_h      = model$synapse_h
  start_from_end = model$start_from_end
  unsync         = model$unsync
  time_y_dim     = model$time_y_dim
  
  # extract the network dimensions, we don't store the binary_dim with the same name as it will be changed later, but we still need it to know what we are doing
  input_dim          = dim(synapse_0)[1]
  output_dim         = dim(synapse_1)[2]
  hidden_dim         = dim(synapse_0)[2]
  time_x_dim_predict = dim(X)[2]
  
  # Storing layers states
  if(unsync == T){
    store_output <- array(0,dim = c(dim(X)[1],dim(X)[2]+time_y_dim,output_dim))
    store_hidden <- array(0,dim = c(dim(X)[1],dim(X)[2]+time_y_dim,hidden_dim))
    # in supplement, we modify the X array so that it will be able to store the futur prediction in the second sequence
    # after the first sequence is passed, if unsync is true, the output become the input, 
    # need a consistency check as the input and output variable dimension must be equals
    store_input <- store_output
    store_input[,1:dim(X)[2],] <- X
    X <- store_input
    rm(store_input) # could be smarter but it will do the trick
  }else{
    store_output <- array(0,dim = c(dim(X)[1:2],output_dim))
    store_hidden <- array(0,dim = c(dim(X)[1:2],hidden_dim))
  }
  
  # extract the binary_dim, needed to be moved as it is now different
  binary_dim = dim(X)[2]
  
  for (j in 1:dim(X)[1]) {
    
    # generate a simple addition problem (a + b = c)
    a = array(X[j,,],dim=c(dim(X)[2],input_dim))
    
    
    layer_1_values = matrix(0, nrow=1, ncol = hidden_dim)
    
    # time index vector, needed because we predict in one direction but update the weight in an other
    if(start_from_end == T){
      pos_vec <- binary_dim:1
      pos_vec_back <- 1:binary_dim
    }else{
      pos_vec <- 1:binary_dim
      pos_vec_back <- binary_dim:1
    }
    
    # moving along the time
    for (position in pos_vec) {
      
      # generate input and output
      x = a[position,]
      
      # hidden layer (input ~+ prev_hidden)
      layer_1 = sigmoid::sigmoid((x%*%synapse_0) + (layer_1_values[dim(layer_1_values)[1],] %*% synapse_h), ...)
      
      # output layer (new binary representation)
      layer_2 = sigmoid::sigmoid(layer_1 %*% synapse_1, ...)
      
      # storing
      store_output[j,position,] = layer_2
      store_hidden[j,position,] = layer_1
      
      # store hidden layer so we can print it out. Needed for error calculation and weight iteration
      layer_1_values = rbind(layer_1_values, layer_1)
      
      # in case of unsync AND second sequence step, we need to assign the output layer values to the next position of a
      if(unsync == T & position >= time_x_dim_predict & position != time_x_dim_predict+time_y_dim){
        a[position+1,] <- layer_2
      }
    }
  }

  
  # return output vector
  if(hidden == FALSE){
    # convert to matrix if 2 dimensional
    if(dim(store_output)[3]==1) {
      store_output <- matrix(store_output,
                       nrow = dim(store_output)[1],
                       ncol = dim(store_output)[2])  }
    # return output
    return(store_output)
  }else{
    return(store_hidden)
  }
}

```

Generate data.

```{r data,fig.height=9,fig.width=7}
# load("data/Propolis silica_dim_ok.Rdata")
load("/home/clau/Dropbox/rnn-master/vignettes/data/Propolis silica_dim_ok.Rdata")
X <- data$chrom[18:34,,]
Y <- X[,126:1,]
# # X <- abind(X,X[,126:1,],along=2)
# X <- array(cbind(X,X[,126:1,]),dim=c(2,126*2,4))
# par(xaxt="n",yaxt="n",mar=c(0,0,0,0))
# layout(matrix(c(4,4,4,4,1,1,2,3),ncol=1))
# X %>% raster
# abline(v=126,col="red")
# text(x=0,y=1.8,labels = "Blue sample",col="red",pos=4,cex=1.5)
# text(x=0,y=0.8,labels = "Orange sample",col="red",pos=4,cex=1.5)
# chrom.pict(aperm(X,c(2,1,3)),1)
# abline(v=0.5,col="red")
# chrom.pict(aperm(X,c(2,1,3)),2)
# abline(v=0.5,col="red")
# abind(data$chrom,data$chrom[,126:1,],along=2) %>% raster
```


```{r training, message=FALSE}
set.seed(1)
model <- trainr(Y = Y,X = X,learningrate = 0.05,hidden_dim = 16,numepochs=100,unsync=T,momentum = 0.1)
```

```{r prediction}
# pred <- predictr(model = model,X = X)
# abind(X,pred,along=2) %>% raster
pred <- predictr(model = model,X = X)
par(mfrow=c(2,1))
abind(X,X[,126:1,],along=2) %>% raster
pred %>% raster

```